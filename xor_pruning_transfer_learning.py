# -*- coding: utf-8 -*-
"""Copy of updated_Transfer_XOR_Net_Notebook_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k_maJHjqiBeGKt8aDV5fpjOr8etdaGdk

# XOR Prediction Neural Network
#### A simple neural network which will learn the XOR logic gate.

## XOR Neural Network

- Input Layer Units = 2
- Hidden Layer Units = 2
- Output Layer Units = 1
"""

import numpy as np # For matrix math
import matplotlib.pyplot as plt # For plotting
import sys # For printing

np.random.seed(100) #for reproducible results

"""### Neural Network Implementation

### Training Data
"""

# The training data.
X = np.array([
    [0, 1],
    [1, 0],
    [1, 1],
    [0, 0]
])

# The labels for the training data.
y = np.array([
    [1],
    [1],
    [0],
    [0]
])

X

y

X = np.repeat(X, 100, axis = 0)
y = np.repeat(y, 100, axis = 0)

print(X.shape, y.shape)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""### Additional Parameters
These are just additional parameters which are required by the weights for their dimensions.
"""

num_i_units = 2 # Number of Input units
num_h_units = 2 # Number of Hidden units
num_o_units = 1 # Number of Output units

"""### Neural Network Parameters
These are the parameters required directly by the NN. Comments should describe the variables.
"""

# The learning rate for Gradient Descent.
learning_rate = 0.01

# The parameter to help with overfitting.
reg_param = 0

# Maximum iterations for Gradient Descent.
max_iter = 3000

# Number of training examples
m = X_train.shape[0]

"""### Weights and Biases

**Weights1(Connection from input to hidden layers)**: num_h_units X num_i_units
**Biases1(Connection from input to hidden layers)**: num_h_units X 1

**Weights2(Connection from hidden to output layers)**: num_o_units X num_h_units
**Biases2(Connection from hidden to output layers)**: num_o_units X 1

### Generating the Weights

### Sigmoid Function
"""

def sigmoid(z, derv=False):
    if derv: return z * (1 - z)
    return 1 / (1 + np.exp(-z))

"""### Forward Propagation

"""

def forward(x, predict=False):
    a1 = x.reshape(x.shape[0], 1) # Getting the training example as a column vector.

    z2 = W1.dot(a1) + B1 # 2x2 * 2x1 + 2x1 = 2x1
    a2 = sigmoid(z2) # 2x1

    z3 = W2.dot(a2) + B2 # 1x2 * 2x1 + 1x1 = 1x1
    a3 = sigmoid(z3)

    if predict: return a3
    return (a1, a2, a3)

"""## Training
This is the training function which contains the means of NN. This contains forward propagation and Backpropagation.
"""

def train(_W1, _W2, _B1, _B2, X, y):

    for i in range(max_iter):
        c = 0

        dW1 = 0
        dW2 = 0

        dB1 = 0
        dB2 = 0

        for j in range(m):
            sys.stdout.write("\rIteration: {} and {}".format(i + 1, j + 1))

            # Forward Prop.
            a0 = X[j].reshape(X[j].shape[0], 1) # 2x1

            z1 = _W1.dot(a0) + _B1 # 2x2 * 2x1 + 2x1 = 2x1
            a1 = sigmoid(z1) # 2x1

            z2 = _W2.dot(a1) + _B2 # 1x2 * 2x1 + 1x1 = 1x1
            a2 = sigmoid(z2) # 1x1

            # Back prop.
            dz2 = a2 - y[j] # 1x1
            dW2 += dz2 * a1.T # 1x1 .* 1x2 = 1x2 # loss

            dz1 = np.multiply((_W2.T * dz2), sigmoid(a1, derv=True)) # (2x1 * 1x1) .* 2x1 = 2x1
            dW1 += dz1.dot(a0.T) # 2x1 * 1x2 = 2x2 #loss

            dB1 += dz1 # 2x1
            dB2 += dz2 # 1x1

            # COST FUNCTION - SIGMOID
            c = c + (-(y[j] * np.log(a2)) - ((1 - y[j]) * np.log(1 - a2)))
            sys.stdout.flush() # Updating the text.

        _W1 = _W1 - learning_rate * (dW1 / m) + ( (reg_param / m) * _W1)
        _W2 = _W2 - learning_rate * (dW2 / m) + ( (reg_param / m) * _W2)

        _B1 = _B1 - learning_rate * (dB1 / m)
        _B2 = _B2 - learning_rate * (dB2 / m)

        cost[i] = (c / m) + (
            (reg_param / (2 * m)) *
            (
                np.sum(np.power(_W1, 2)) +
                np.sum(np.power(_W2, 2))
            )
        )

        # print(cost)
    return (_W1, _W2, _B1, _B2)

# np.random.seed(1)
W1 = np.random.normal(0, 1, (num_h_units, num_i_units)) # 2x2
W2 = np.random.normal(0, 1, (num_o_units, num_h_units)) # 1x2

B1 = np.random.random((num_h_units, 1)) # 2x1
B2 = np.random.random((num_o_units, 1)) # 1x1

max_iter = 3000

dW1 = 0 # Gradient for W1 (change of weights)
dW2 = 0 # Gradient for W2 (change of weights)

dB1 = 0 # Gradient for B1
dB2 = 0 # Gradient for B2

cost = np.zeros((max_iter, 1)) # Column vector to record the cost of the NN after each Gradient Descent iteration.

W1, W2, B1, B2 = train(W1, W2, B1, B2, X_train, y_train)

W1

W2

B1

B2

"""### Plotting
Now, let's plot a simple plot showing the cost function with respect to the number of iterations of gradient descent.
"""

# Assigning the axes to the different elements.
plt.plot(range(max_iter), cost)

# Labelling the x axis as the iterations axis.
plt.xlabel("Iterations")

# Labelling the y axis as the cost axis.
plt.ylabel("Cost")

# Showing the plot.
plt.show()

"""Testing"""

def test(_W1, _W2, _B1, _B2, X):

    outputs = []

    for j in range(len(X)):
        # Forward Prop.
        a0 = X[j].reshape(X[j].shape[0], 1) # 2x1

        z1 = _W1.dot(a0) + _B1 # 2x2 * 2x1 + 2x1 = 2x1
        a1 = sigmoid(z1) # 2x1

        z2 = _W2.dot(a1) + _B2 # 1x2 * 2x1 + 1x1 = 1x1
        a2 = sigmoid(z2) # 1x1

        outputs.append(a2[0])
    return outputs

print(W1, W2)

pred_y = test(W1, W2, B1, B2, X_test)

from sklearn.metrics import classification_report

pred_y = np.array(pred_y)
pred_y = (pred_y > 0.5) * 1

print(classification_report(pred_y, y_test, zero_division = 1))

"""## Pruning"""

W1prune, W2prune = np.array(W1), np.array(W2)

costprune1 = dict()
costprune2 = dict()

print(W1prune, W2prune)

def get_cost(_W1, _W2, _B1, _B2, X, y):

  # Forward Prop.
  a0 = X[j].reshape(X[j].shape[0], 1) # 2x1

  z1 = _W1.dot(a0) + _B1 # 2x2 * 2x1 + 2x1 = 2x1
  a1 = sigmoid(z1) # 2x1

  z2 = _W2.dot(a1) + _B2 # 1x2 * 2x1 + 1x1 = 1x1
  a2 = sigmoid(z2) # 1x1

  # COST FUNCTION - SIGMOID
  return (-(y[j] * np.log(a2)) - ((1 - y[j]) * np.log(1 - a2)))

for i in range(len(W1prune)):
  for j in range(len(W1prune)):
    temp = W1prune[i, j]
    W1prune[i, j] = 0
    costprune1[(i,j)] = get_cost(W1prune, W2prune, B1, B2, X, y)
    W1prune[i, j] = temp

for i in range(len(W2prune)):
  for j in range(2):
    temp = W1prune[i, j]
    W1prune[i, j] = 0
    costprune2[(i,j)] = get_cost(W1prune, W2prune, B1, B2, X, y)
    W1prune[i, j] = temp

costprune1

costprune2

"""Now pruning"""

W1prune[0, 1] = 0
# W1prune[1, 1] = 0

W2prune[0, 1] = 0

print(W1prune, W2prune)

print(f'Cost before pruning = {get_cost(W1, W2, B1, B2, X, y)[0][0]}')

print(f'Final cost after pruning = {get_cost(W1prune, W2prune, B1, B2, X, y)[0][0]}')

W1prunefinal = W1prune
W2prunefinal = W2prune

pred_y = test(W1prune, W2prune, B1, B2, X_test)

from sklearn.metrics import classification_report

pred_y = np.array(pred_y)
pred_y = (pred_y > 0.5) * 1

print(classification_report(pred_y, y_test, zero_division = 1))

"""##Transfer Learning on Or gate

##Creating a copy of original weights
"""

W1c = np.array(W1)
W2c = np.array(W2)

# The training data.
X = np.array([
    [0, 1],
    [1, 0],
    [1, 1],
    [0, 0]
])

# The labels for the training data.
y = np.array([
    [1],
    [1],
    [1],
    [0]
])

X = np.repeat(X, 100, axis = 0)
y = np.repeat(y, 100, axis = 0)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""#Without pruning results"""

# Maximum iterations for Gradient Descent.
max_iter = 500

dW1 = 0 # Gradient for W1
dW2 = 0 # Gradient for W2

dB1 = 0 # Gradient for B1
dB2 = 0 # Gradient for B2

B1 = np.random.random((num_h_units, 1)) # 2x1
B2 = np.random.random((num_o_units, 1)) # 1x1

cost = np.zeros((max_iter, 1)) # Column vector to record the cost of the NN after each Gradient Descent iteration.

W1c, W2c, B1, B2 = train(W1c, W2c, B1, B2, X_train, y_train)

# Assigning the axes to the different elements.
plt.plot(range(max_iter), cost)

# Labelling the x axis as the iterations axis.
plt.xlabel("Iterations")

# Labelling the y axis as the cost axis.
plt.ylabel("Cost")

# Showing the plot.
plt.show()

print(f'Final value before pruning : {cost[-1]}')

pred_y = test(W1c, W2c, B1, B2, X_test)

from sklearn.metrics import classification_report

pred_y = np.array(pred_y)
pred_y = (pred_y > 0.5) * 1

print(classification_report(pred_y, y_test))

"""With pruning"""

W1prune = np.array(W1prunefinal)
W2prune = np.array(W2prunefinal)

# Maximum iterations for Gradient Descent.
max_iter = 500

dW1 = 0 # Gradient for W1
dW2 = 0 # Gradient for W2

dB1 = 0 # Gradient for B1
dB2 = 0 # Gradient for B2

B1 = np.random.random((num_h_units, 1)) # 2x1
B2 = np.random.random((num_o_units, 1)) # 1x1

cost = np.zeros((max_iter, 1)) # Column vector to record the cost of the NN after each Gradient Descent iteration.

W1prune, W2prune, B1, B2 = train(W1prune, W2prune, B1, B2, X_train, y_train)

# Assigning the axis to the different elements.
plt.plot(range(max_iter), cost)

# Labelling the x axis as the iterations axis.
plt.xlabel("Iterations")

# Labelling the y axis as the cost axis.
plt.ylabel("Cost")

# Showing the plot.
plt.show()

print(f'Final value after pruning : {cost[-1]}')

pred_y = test(W1prune, W2prune, B1, B2, X_test)

from sklearn.metrics import classification_report

pred_y = np.array(pred_y)
pred_y = (pred_y > 0.5) * 1

print(classification_report(pred_y, y_test))

"""##Transfer Learning on AND gate"""

W1c = np.array(W1)
W2c = np.array(W2)

# The training data.
X = np.array([
    [0, 1],
    [1, 0],
    [1, 1],
    [0, 0]
])

# The labels for the training data.
y = np.array([
    [0],
    [0],
    [1],
    [0]
])

X = np.repeat(X, 100, axis = 0)
y = np.repeat(y, 100, axis = 0)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Maximum iterations for Gradient Descent.
max_iter = 500

dW1 = 0 # Gradient for W1
dW2 = 0 # Gradient for W2

dB1 = 0 # Gradient for B1
dB2 = 0 # Gradient for B2

B1 = np.random.random((num_h_units, 1)) # 2x1
B2 = np.random.random((num_o_units, 1)) # 1x1

cost = np.zeros((max_iter, 1)) # Column vector to record the cost of the NN after each Gradient Descent iteration.
W1c, W2c, B1, B2 = train(W1c, W2c, B1, B2, X_train, y_train)

# Assigning the axes to the different elements.
plt.plot(range(max_iter), cost)

# Labelling the x axis as the iterations axis.
plt.xlabel("Iterations")

# Labelling the y axis as the cost axis.
plt.ylabel("Cost")

# Showing the plot.
plt.show()

print(f'Final value before pruning : {cost[-1]}')

pred_y = test(W1c, W2c, B1, B2, X_test)

from sklearn.metrics import classification_report

pred_y = np.array(pred_y)
pred_y = (pred_y > 0.5) * 1

print(classification_report(pred_y, y_test))

"""#with pruning"""

W1prune = np.array(W1prunefinal)
W2prune = np.array(W2prunefinal)

# Maximum iterations for Gradient Descent.
max_iter = 500

dW1 = 0 # Gradient for W1
dW2 = 0 # Gradient for W2

dB1 = 0 # Gradient for B1
dB2 = 0 # Gradient for B2

B1 = np.random.random((num_h_units, 1)) # 2x1
B2 = np.random.random((num_o_units, 1)) # 1x1

cost = np.zeros((max_iter, 1)) # Column vector to record the cost of the NN after each Gradient Descent iteration.

W1prune, W2prune, B1, B2 = train(W1prune, W2prune, B1, B2, X_train, y_train)

# Assigning the axis to the different elements.
plt.plot(range(max_iter), cost)

# Labelling the x axis as the iterations axis.
plt.xlabel("Iterations")

# Labelling the y axis as the cost axis.
plt.ylabel("Cost")

# Showing the plot.
plt.show()

print(f'Final value after pruning : {cost[-1]}')

pred_y = test(W1prune, W2prune, B1, B2, X_test)

from sklearn.metrics import classification_report

pred_y = np.array(pred_y)
pred_y = (pred_y > 0.5) * 1

print(classification_report(pred_y, y_test))

